# Progress for ollama/ollama (2024-10-20 to 2024-10-27)


## Issues Closed in the Last 7 Days
- Bump to latest Go 1.22 patch #7379
- HTTP generates API and returns 500 codes within a fixed one minute timeframe #7373
- Fix deepseek deseret regex #7369
- CI testing #7367
- Add AirLLM or similar to allow running big models with low RAM #7366
- Unable to pull IQ4_NL quants from HF #7365
- Fix incremental build file deps #7361
- Fix unicode output on windows with redirect to file #7358
- Released binaries have High severity CVEs due to Go version 1.22.5 #7355
- Ollama does not run on GPU at 0.4.0-rc5-rocm version  #7346
- Improve dependency gathering logic #7345
- Aya-Expanse 32B & 8B #7343
- Add 'prefix' option to chat endpoint #7342
- Ollama forgets previous information in conversation if a prompt sent by the user is very large #7341
- Is it possible to use the API to pass my documents to the model and have it understand them in depth?  #7340
- Error: llama runner process has terminated: signal: aborted (core dumped) #7334
- `OLLAMA_MODELS` env var is ignored #7333
- Why Does Ollama Use Shard GPU Memory Before Filling Dedicated Ones? #7330
- Terminate the current task after the REST request is actively ended #7329
- Performance degradation with 8B+ models on Windows Radeon #7328
- Support loading the same model more than once #7321
- 0.4.0 regression #7320
- support LLaMA-Omni #7319
- ollama won't start as a service, will start using 'serve'? #7317
- ollama run llamaX.X The directory where the model is downloaded and stored #7314
- ollama 0.4.0-rc3: deepseek-coder-v2-lite is not functioning correctly. #7311
- ollama run hf.co/* does not use Modelfile in repo  #7307
- integration: harden embedding test #7306
- Fix rocm windows build and clean up dependency gathering #7305
- runner.go: Merge partial unicode characters before sending #7303
